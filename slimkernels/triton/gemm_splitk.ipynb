{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "@triton.jit()\n",
    "def swizzle_tile(pid,\n",
    "                m, n,\n",
    "                block_m: tl.constexpr, block_n: tl.constexpr, group_m: tl.constexpr):\n",
    "    \n",
    "    grid_m = tl.cdiv(m, block_m)\n",
    "    grid_n = tl.cdiv(n, block_n)\n",
    "\n",
    "    width = group_m * grid_n\n",
    "    group_id = pid // width\n",
    "    group_size = tl.minimum(grid_m - group_id * group_m, group_m)\n",
    "\n",
    "    pid_m = group_id * group_m + (pid % group_size)\n",
    "    pid_n = (pid % width) // group_size\n",
    "\n",
    "    return pid_m, pid_n\n",
    "\n",
    "def w4matmul_splitk_autotune_config():\n",
    "    configs = []\n",
    "    for block_size_m in [32]:\n",
    "        for block_size_n in [64]:\n",
    "            for block_size_k in [64]:\n",
    "                for num_stages in [4]:\n",
    "                    for num_warps in [4]:\n",
    "                        configs.append(triton.Config({'BLOCK_SIZE_M': block_size_m, 'BLOCK_SIZE_N': block_size_n, 'BLOCK_SIZE_K': block_size_k, 'GROUP_SIZE_M': 8, 'SPLIT_K': 4}, num_stages=num_stages,\n",
    "                      num_warps=num_warps))\n",
    "    return configs\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=w4matmul_splitk_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def w4matmul_splitk_kernel(\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "        GROUP_SIZE_M: tl.constexpr,  SPLIT_K: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    pid_k = tl.program_id(1)\n",
    "    total_blocks_k = tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)\n",
    "    pid_m, pid_n = swizzle_tile(pid, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M)\n",
    "\n",
    "    #Offsets\n",
    "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    #Vectorized coalesced load\n",
    "    ##############################\n",
    "    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, BLOCK_SIZE_M), BLOCK_SIZE_M)\n",
    "    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, BLOCK_SIZE_N), BLOCK_SIZE_N)\n",
    "    ###############################\n",
    "    \n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + ((offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "    shifter = (offs_k % 8) * 4\n",
    "    \n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, total_blocks_k):\n",
    "        a = tl.load(a_ptrs)\n",
    "        b = tl.load(b_ptrs)\n",
    "        b = ((b >> shifter[:, None]) & 0xF).to(tl.float16)\n",
    "\n",
    "        accumulator += tl.dot(a, b)\n",
    "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n",
    "        b_ptrs += (BLOCK_SIZE_K // 8) * SPLIT_K * stride_bk\n",
    "    \n",
    "    accumulator.to(tl.float16)\n",
    "\n",
    "    offs_cm = pid_m*BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n*BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n",
    "    tl.atomic_add(c_ptrs, accumulator, sem='release')\n",
    "    \n",
    "\n",
    "def w4matmul_splitk(a, b, c):\n",
    "    # Check constraints.\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    _, N = b.shape\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), META['SPLIT_K'])\n",
    "    w4matmul_splitk_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2matmul_autotune_config():\n",
    "    configs = []\n",
    "    for block_size_m in [32]:\n",
    "        for block_size_n in [64]:\n",
    "            for block_size_k in [64]:\n",
    "                for num_stages in [4]:\n",
    "                    for num_warps in [4]:\n",
    "                        configs.append(triton.Config({'BLOCK_SIZE_M': block_size_m, 'BLOCK_SIZE_N': block_size_n, 'BLOCK_SIZE_K': block_size_k, 'GROUP_SIZE_M': 8,}, num_stages=num_stages,\n",
    "                      num_warps=num_warps))\n",
    "    return configs\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=w2matmul_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def w2matmul_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    \n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    \n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n//8 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    \n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        b = ((b>>4) & 0b11).to(tl.float16)\n",
    "        accumulator = tl.dot(a, b, accumulator)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    \n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N*8)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "def w2matmul(a, b, c):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N*8, META['BLOCK_SIZE_N']), )\n",
    "    w2matmul_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"M\", \"N\", \"K\"],  # Argument names to use as an x-axis for the plot\n",
    "            x_vals=[((2**i), 2048, 1024) for i in range(6,7)],  # Different possible values for `x_name`\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            # Possible values for `line_arg`\n",
    "            # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "            line_vals=[\"torch_fp16\", \"triton_wint2_base\", \"triton_wint4_splitk\"],  # Label name for the lines\n",
    "            line_names=[\"torch_fp16\", \"triton_wint2_base\", \"triton_wint4_splitk\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"red\", \"-\"), (\"blue\", \"-\")],\n",
    "            ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "            plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "            args={},\n",
    "        ))\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    c = torch.zeros((M, N), device=a.device, dtype=torch.float16)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == \"torch_fp16\":\n",
    "        b = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    if provider == 'triton_wint2_base':\n",
    "        packed_b = torch.randint(0, 256, (K, N//8),device=DEVICE, dtype=torch.uint16)\n",
    "        packed_b = packed_b.T.contiguous()\n",
    "        packed_b = packed_b.T\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: w2matmul(a, packed_b, c), quantiles=quantiles)\n",
    "    if provider == 'triton_wint4_splitk':\n",
    "        packed_b = torch.randint(0, 256, (K//8, N),device=DEVICE, dtype=torch.uint32)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: w4matmul_splitk(a, packed_b, c), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRITON_PRINT_AUTOTUNING=1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%set_env TRITON_PRINT_AUTOTUNING=1\n",
    "%env TRITON_PRINT_AUTOTUNING\n",
    "benchmark.run(show_plots=False, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
